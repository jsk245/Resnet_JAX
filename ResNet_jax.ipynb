{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsk245/Resnet_JAX/blob/'main'/ResNet_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ldef4cBUoQh",
        "outputId": "73fd9940-dea1-439b-fc9a-34713206b7d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzZmVEQ7XI6O",
        "outputId": "6236dbfc-2bd4-4b8f-8e59-a928e735af00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dm-haiku in /usr/local/lib/python3.7/dist-packages (0.0.7)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (1.21.6)\n",
            "Requirement already satisfied: jmp>=0.0.2 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (4.1.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (1.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.8.10)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.14+cuda11.cudnn805)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax) (0.1.3)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.14)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.7)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (0.6.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.1.55->optax) (5.8.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.1.55->optax) (3.8.1)\n"
          ]
        }
      ],
      "source": [
        "pip install dm-haiku optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykRpYWuyohKr",
        "outputId": "340bb54d-9440-42e0-b4ba-7ea10ad15f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version 0.3.14\n",
            "Haiku version 0.0.7\n",
            "TF version 2.8.2\n"
          ]
        }
      ],
      "source": [
        "import haiku as hk\n",
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tf.enable_v2_behavior()\n",
        "tf.config.set_visible_devices([], device_type='GPU')\n",
        "\n",
        "print(\"JAX version {}\".format(jax.__version__))\n",
        "print(\"Haiku version {}\".format(hk.__version__))\n",
        "print(\"TF version {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lGuFU-DlFMnN"
      },
      "outputs": [],
      "source": [
        "data_dir = '/tmp/tfds'\n",
        "\n",
        "# Fetch full datasets for evaluation\n",
        "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
        "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
        "cifar100_data = tfds.load(name=\"cifar100\", data_dir=data_dir, split=\"train\")\n",
        "\n",
        "def make_dataset(batch_size, seed=1):\n",
        "  def _preprocess(sample):\n",
        "    # Convert to floats in [0, 1].\n",
        "    image = tf.image.convert_image_dtype(sample[\"image\"], tf.float32)\n",
        "    # Scale the data to [-1, 1] to stabilize training.\n",
        "    return 2.0 * image - 1.0\n",
        "  def _label_identity(sample):\n",
        "    label = sample['label']\n",
        "    return label\n",
        "\n",
        "  ds = cifar100_data\n",
        "  #ds = cifar100_data[\"train\"]\n",
        "  ds = ds.map(map_func=_preprocess, \n",
        "              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  ds = ds.cache()\n",
        "  ds = ds.shuffle(100 * batch_size, seed=seed).repeat().batch(batch_size)\n",
        "\n",
        "  labels = cifar100_data\n",
        "  #labels = cifar100_data[\"train\"]\n",
        "  labels = labels.map(map_func=_label_identity, \n",
        "              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  labels = labels.cache()\n",
        "  labels = labels.shuffle(100 * batch_size, seed=seed).repeat().batch(batch_size)\n",
        "  return (iter(tfds.as_numpy(ds)), iter(tfds.as_numpy(labels)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save(ckpt_dir: str, state) -> None:\n",
        " with open(os.path.join(ckpt_dir, \"arrays.npy\"), \"wb\") as f:\n",
        "   for x in jax.tree_leaves(state):\n",
        "     np.save(f, x, allow_pickle=False)\n",
        "\n",
        " tree_struct = jax.tree_map(lambda t: 0, state)\n",
        " with open(os.path.join(ckpt_dir, \"tree.pkl\"), \"wb\") as f:\n",
        "   pickle.dump(tree_struct, f)\n",
        "\n",
        "def restore(ckpt_dir):\n",
        " with open(os.path.join(ckpt_dir, \"tree.pkl\"), \"rb\") as f:\n",
        "   tree_struct = pickle.load(f)\n",
        " \n",
        " leaves, treedef = jax.tree_flatten(tree_struct)\n",
        " with open(os.path.join(ckpt_dir, \"arrays.npy\"), \"rb\") as f:\n",
        "   flat_state = [np.load(f) for _ in leaves]\n",
        "\n",
        " return jax.tree_unflatten(treedef, flat_state)"
      ],
      "metadata": {
        "id": "Z-dHAB0taj72"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to restore stored model\n",
        "params = restore(\"/content/gdrive/MyDrive/ResNet_JAX/params\")\n",
        "state = restore(\"/content/gdrive/MyDrive/ResNet_JAX/model_state\")\n",
        "with open(os.path.join(\"/content/gdrive/MyDrive/ResNet_JAX/opt_state\", \"opt_state.pkl\"), \"rb\") as input_file:\n",
        "  opt_state = pickle.load(input_file)"
      ],
      "metadata": {
        "id": "ewCPW7s-arDI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q3Eyr84QZOh3"
      },
      "outputs": [],
      "source": [
        "class ConvAndBatchNormModule(hk.Module):\n",
        "  def __init__(self, is_training, outchannels, kernel_size, stride, name=None):\n",
        "    super(ConvAndBatchNormModule, self).__init__(name=name)\n",
        "    self.conv = hk.Conv2D(outchannels, kernel_size, stride)\n",
        "    self.bn = hk.BatchNorm(True, True, 0.9, cross_replica_axis=\"jax.vmap\")\n",
        "    self.is_training = is_training\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.bn(x, self.is_training)\n",
        "    return x\n",
        "\n",
        "class ResModule(hk.Module):\n",
        "  def __init__(self, is_training, inchannels, adjust_dimension, name=None):\n",
        "    super(ResModule, self).__init__(name=name)\n",
        "    outchannels = inchannels // 4\n",
        "\n",
        "    self.dimensionHelper = None\n",
        "    if adjust_dimension:\n",
        "      self.dimensionHelper = ResDimensionHelper()\n",
        "      outchannels = 2 * outchannels\n",
        "      inchannels = 2 * inchannels\n",
        "\n",
        "    self.conv1 = ConvAndBatchNormModule(is_training, outchannels, 1, 1)\n",
        "\n",
        "    if adjust_dimension:\n",
        "      self.conv2 = ConvAndBatchNormModule(is_training, outchannels, 3, 2)\n",
        "    else:\n",
        "      self.conv2 = ConvAndBatchNormModule(is_training, outchannels, 3, 1)\n",
        "\n",
        "    self.conv3 = ConvAndBatchNormModule(is_training, inchannels, 1, 1)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x_res = x\n",
        "    x = jax.nn.relu(self.conv1(x))\n",
        "    x = jax.nn.relu(self.conv2(x))\n",
        "    x = self.conv3(x)\n",
        "    if self.dimensionHelper != None:\n",
        "      x_res = self.dimensionHelper(x_res)\n",
        "    x = x + x_res\n",
        "    x = jax.nn.relu(x)\n",
        "    return x\n",
        "\n",
        "class ResDimensionHelper(hk.Module):\n",
        "  def __init__(self, name=None):\n",
        "    super(ResDimensionHelper, self).__init__(name=name)\n",
        "    self.maxPool = hk.MaxPool(2, [2,2,1], \"SAME\")\n",
        "\n",
        "  def __call__(self, x):\n",
        "    added_channels = x.shape[2] // 2\n",
        "    x = self.maxPool(x)\n",
        "    x = jnp.pad(x, ((0,0),(0,0),(added_channels, added_channels)))\n",
        "    return x\n",
        "\n",
        "class DownSampleModule(hk.Module):\n",
        "  def __init__(self, name=None):\n",
        "    super(DownSampleModule, self).__init__(name=name)\n",
        "    self.conv = hk.Conv2D(64, 7, 2, padding=\"VALID\")\n",
        "    self.maxPool = hk.MaxPool(3, [2,2,1], \"SAME\")\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = jnp.pad(x, ((3,3),(3,3),(0, 0)))\n",
        "    x = self.conv(x)\n",
        "    x = self.maxPool(x)\n",
        "    return x\n",
        "\n",
        "class GlobalPoolAndFCModule(hk.Module):\n",
        "  def __init__(self, goal_num_classes, name=None):\n",
        "    super(GlobalPoolAndFCModule, self).__init__(name=name)\n",
        "    self.flatten = hk.Flatten(preserve_dims=-3)\n",
        "    self.linear = hk.Linear(goal_num_classes)\n",
        "  def __call__(self, x):\n",
        "    x = hk.avg_pool(x, [x.shape[0], x.shape[1], 1], 1, 'VALID')\n",
        "    x = self.flatten(x)\n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "def softmax_cross_entropy(logits, labels):\n",
        "  one_hot = jax.nn.one_hot(labels, logits.shape[-1])\n",
        "  return jnp.mean(-jnp.sum(jax.nn.log_softmax(logits) * one_hot, axis=-1))\n",
        "\n",
        "def accuracy(logits, labels):\n",
        "  logits = jnp.argmax(logits, axis=1)\n",
        "  return jnp.mean(logits == labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RF6QbVfQJD-1"
      },
      "outputs": [],
      "source": [
        "def forward(data, labels, is_training):\n",
        "\n",
        "  my_func = hk.Sequential([DownSampleModule(),\n",
        "                           hk.Conv2D(256, 1, 1),\n",
        "                           ResModule(is_training, 256, False),\n",
        "                           ResModule(is_training, 256, False),\n",
        "                           ResModule(is_training, 256, False),\n",
        "                           ResModule(is_training, 256, True),\n",
        "                           ResModule(is_training, 512, False),\n",
        "                           ResModule(is_training, 512, False),\n",
        "                           ResModule(is_training, 512, False),\n",
        "                           ResModule(is_training, 512, True),\n",
        "                           ResModule(is_training, 1024, False),\n",
        "                           ResModule(is_training, 1024, False),\n",
        "                           ResModule(is_training, 1024, False),\n",
        "                           ResModule(is_training, 1024, False),\n",
        "                           ResModule(is_training, 1024, False),\n",
        "                           ResModule(is_training, 1024, True),\n",
        "                           ResModule(is_training, 2048, False),\n",
        "                           ResModule(is_training, 2048, False),\n",
        "                           GlobalPoolAndFCModule(100)])\n",
        "  logits = jax.vmap(my_func, axis_name=\"jax.vmap\")(data)\n",
        "  loss = softmax_cross_entropy(logits, labels)\n",
        "  acc = accuracy(logits, labels)\n",
        "  return {\"loss\": loss, \"accuracy\": acc}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Xypa68fwRAtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4828e77-7a69-4029-e843-2ff3dcdfc662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py:598: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.get_single_element()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py:598: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.get_single_element()`.\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.01\n",
        "input_data, input_labels = make_dataset(1024, seed=1)\n",
        "test_set = tfds.load(name=\"cifar100\", data_dir=data_dir, split=\"test\", batch_size=-1)\n",
        "test_data, test_labels = tfds.as_numpy(test_set[\"image\"]), tfds.as_numpy(test_set[\"label\"])\n",
        "forward = hk.transform_with_state(forward)\n",
        "optimizer = optax.adam(learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sample):\n",
        "    # Convert to floats in [0, 1].\n",
        "    image = tf.image.convert_image_dtype(sample, tf.float32)\n",
        "    # Scale the data to [-1, 1] to stabilize training.\n",
        "    return 2.0 * image - 1.0\n",
        "test_set = tfds.load(name=\"cifar100\", data_dir=data_dir, split=\"test\", batch_size=-1)\n",
        "test_data, test_labels = tfds.as_numpy(preprocess(test_set[\"image\"])), tfds.as_numpy(test_set[\"label\"])"
      ],
      "metadata": {
        "id": "Pbye_jNlpTK3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PKejg_PDRdeY"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(params, state, opt_state, data, labels):\n",
        "  def adapt_forward(params, state, data, labels):\n",
        "    # Pack model output and state together.\n",
        "    model_output, state = forward.apply(params, state, None, data, labels, True)\n",
        "    loss = model_output[\"loss\"]\n",
        "    return loss, (model_output, state)\n",
        "\n",
        "  grads, (model_output, state) = (jax.grad(adapt_forward, has_aux=True)(params, state, data, labels))\n",
        "\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "\n",
        "  return params, state, opt_state, model_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7UaYPYSRwJo",
        "outputId": "cbfb8e69-eb61-425f-98c5-9e976e251871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 100/2000] train loss: 5.473975 train accuracy: 0.029131 test accuracy: 0.066400 \n",
            "[Step 200/2000] train loss: 3.924376 train accuracy: 0.098184 test accuracy: 0.134600 \n",
            "[Step 300/2000] train loss: 3.530784 train accuracy: 0.157988 test accuracy: 0.178200 \n",
            "[Step 400/2000] train loss: 3.252978 train accuracy: 0.207188 test accuracy: 0.213800 \n",
            "[Step 500/2000] train loss: 3.021037 train accuracy: 0.253535 test accuracy: 0.255500 \n",
            "[Step 600/2000] train loss: 2.791674 train accuracy: 0.296182 test accuracy: 0.258900 \n",
            "[Step 700/2000] train loss: 2.571781 train accuracy: 0.339492 test accuracy: 0.301900 \n",
            "[Step 800/2000] train loss: 2.340767 train accuracy: 0.388838 test accuracy: 0.310800 \n",
            "[Step 900/2000] train loss: 2.084434 train accuracy: 0.444258 test accuracy: 0.317800 \n",
            "[Step 1000/2000] train loss: 1.819685 train accuracy: 0.501826 test accuracy: 0.329700 \n",
            "[Step 1100/2000] train loss: 1.508977 train accuracy: 0.575645 test accuracy: 0.324600 \n",
            "[Step 1200/2000] train loss: 1.188191 train accuracy: 0.653291 test accuracy: 0.325000 \n",
            "[Step 1300/2000] train loss: 0.885691 train accuracy: 0.732695 test accuracy: 0.330800 \n",
            "[Step 1400/2000] train loss: 0.634734 train accuracy: 0.801719 test accuracy: 0.331600 \n",
            "[Step 1500/2000] train loss: 0.441160 train accuracy: 0.858760 test accuracy: 0.324500 \n",
            "[Step 1600/2000] train loss: 0.327304 train accuracy: 0.893652 test accuracy: 0.326600 \n",
            "[Step 1700/2000] train loss: 0.249329 train accuracy: 0.918252 test accuracy: 0.320500 \n",
            "[Step 1800/2000] train loss: 0.201390 train accuracy: 0.934482 test accuracy: 0.330300 \n",
            "[Step 1900/2000] train loss: 0.177456 train accuracy: 0.941562 test accuracy: 0.336000 \n",
            "[Step 2000/2000] train loss: 0.169720 train accuracy: 0.943457 test accuracy: 0.337700 \n"
          ]
        }
      ],
      "source": [
        "num_training_updates = 2000\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "rng = jax.random.PRNGKey(42)\n",
        "#with jax.checking_leaks():\n",
        "params, state = forward.init(rng, next(input_data), next(input_labels), True)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "\n",
        "for step in range(1, num_training_updates + 1):\n",
        "  data = next(input_data)\n",
        "  labels = next(input_labels)\n",
        "  params, state, opt_state, train_results = (train_step(params, state, opt_state, data, labels))\n",
        "\n",
        "  train_results = jax.device_get(train_results)\n",
        "  train_losses.append(train_results[\"loss\"])\n",
        "  train_accuracies.append(train_results[\"accuracy\"])\n",
        "\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    model_output, _ = forward.apply(params, state, None, test_data, test_labels, False)\n",
        "    train_step._clear_cache()\n",
        "    #forward.init(rng, next(input_data), next(input_labels), True)\n",
        "\n",
        "    print(f'[Step {step}/{num_training_updates}] ' + \n",
        "          ('train loss: %f ' % np.mean(train_losses[-100:])) + \n",
        "          ('train accuracy: %f ' % np.mean(train_accuracies[-100:])) + \n",
        "          ('test accuracy: %f ' % model_output[\"accuracy\"]))\n",
        "    if step % 1000 == 0:\n",
        "      save(\"/content/gdrive/MyDrive/ResNet_JAX/params\", params)\n",
        "      save(\"/content/gdrive/MyDrive/ResNet_JAX/model_state\", state)\n",
        "      with open(os.path.join(\"/content/gdrive/MyDrive/ResNet_JAX/opt_state\", \"opt_state.pkl\"), \"wb\") as output_file:\n",
        "        pickle.dump(opt_state, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for loaded in params/states\n",
        "\n",
        "num_training_updates = 2000\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "#rng = jax.random.PRNGKey(42)\n",
        "#with jax.checking_leaks():\n",
        "#params, state = forward.init(rng, next(input_data), next(input_labels), True)\n",
        "#opt_state = optimizer.init(params)\n",
        "\n",
        "\n",
        "for step in range(1, num_training_updates + 1):\n",
        "  data = next(input_data)\n",
        "  labels = next(input_labels)\n",
        "  params, state, opt_state, train_results = (train_step(params, state, opt_state, data, labels))\n",
        "\n",
        "  train_results = jax.device_get(train_results)\n",
        "  train_losses.append(train_results[\"loss\"])\n",
        "  train_accuracies.append(train_results[\"accuracy\"])\n",
        "\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    model_output, _ = forward.apply(params, state, None, test_data, test_labels, False)\n",
        "    train_step._clear_cache()\n",
        "\n",
        "    print(f'[Step {step}/{num_training_updates}] ' + \n",
        "          ('train loss: %f ' % np.mean(train_losses[-100:])) + \n",
        "          ('train accuracy: %f ' % np.mean(train_accuracies[-100:])) + \n",
        "          ('test accuracy: %f ' % model_output[\"accuracy\"]))\n",
        "    if step % 1000 == 0:\n",
        "      save(\"/content/gdrive/MyDrive/ResNet_JAX/params\", params)\n",
        "      save(\"/content/gdrive/MyDrive/ResNet_JAX/model_state\", state)\n",
        "      with open(os.path.join(\"/content/gdrive/MyDrive/ResNet_JAX/opt_state\", \"opt_state.pkl\"), \"wb\") as output_file:\n",
        "        pickle.dump(opt_state, output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfTXlC0bdc_M",
        "outputId": "325e1ecd-a52a-4e7a-f65f-f1a442ab5207"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 100/2000] train loss: 0.161349 train accuracy: 0.946885 test accuracy: 0.333100 \n",
            "[Step 200/2000] train loss: 0.182527 train accuracy: 0.940645 test accuracy: 0.330100 \n",
            "[Step 300/2000] train loss: 0.170258 train accuracy: 0.944746 test accuracy: 0.321500 \n",
            "[Step 400/2000] train loss: 0.170675 train accuracy: 0.944570 test accuracy: 0.326000 \n",
            "[Step 500/2000] train loss: 0.145793 train accuracy: 0.952207 test accuracy: 0.333800 \n",
            "[Step 600/2000] train loss: 0.103779 train accuracy: 0.966123 test accuracy: 0.338800 \n",
            "[Step 700/2000] train loss: 0.093092 train accuracy: 0.969717 test accuracy: 0.338200 \n",
            "[Step 800/2000] train loss: 0.113038 train accuracy: 0.963779 test accuracy: 0.327700 \n",
            "[Step 900/2000] train loss: 0.173540 train accuracy: 0.944326 test accuracy: 0.323900 \n",
            "[Step 1000/2000] train loss: 0.226762 train accuracy: 0.929219 test accuracy: 0.331600 \n",
            "[Step 1100/2000] train loss: 0.218022 train accuracy: 0.932734 test accuracy: 0.336000 \n",
            "[Step 1200/2000] train loss: 0.137429 train accuracy: 0.956191 test accuracy: 0.336400 \n",
            "[Step 1300/2000] train loss: 0.102461 train accuracy: 0.967217 test accuracy: 0.341400 \n",
            "[Step 1400/2000] train loss: 0.085254 train accuracy: 0.972363 test accuracy: 0.343100 \n",
            "[Step 1500/2000] train loss: 0.074215 train accuracy: 0.976318 test accuracy: 0.338400 \n",
            "[Step 1600/2000] train loss: 0.107132 train accuracy: 0.966436 test accuracy: 0.334900 \n",
            "[Step 1700/2000] train loss: 0.180900 train accuracy: 0.945420 test accuracy: 0.331200 \n",
            "[Step 1800/2000] train loss: 0.205167 train accuracy: 0.938799 test accuracy: 0.329300 \n",
            "[Step 1900/2000] train loss: 0.200960 train accuracy: 0.939873 test accuracy: 0.321800 \n",
            "[Step 2000/2000] train loss: 0.153823 train accuracy: 0.953740 test accuracy: 0.328400 \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ResNet_jax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMM2ooNvAVr5pae7vmaUkew",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}