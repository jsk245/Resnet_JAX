{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsk245/Resnet_JAX/blob/'main'/Resnet_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzZmVEQ7XI6O",
        "outputId": "8e05ac21-467a-44d1-c097-a733632f30cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dm-haiku in /usr/local/lib/python3.7/dist-packages (0.0.7)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (4.1.1)\n",
            "Requirement already satisfied: jmp>=0.0.2 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.0.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (1.21.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.8.10)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (1.1.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.14+cuda11.cudnn805)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.14)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax) (0.1.3)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.7)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (0.6.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (1.7.3)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.1.55->optax) (5.8.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.1.55->optax) (3.8.0)\n"
          ]
        }
      ],
      "source": [
        "pip install dm-haiku optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykRpYWuyohKr",
        "outputId": "a55419a3-2ea9-4a26-ab0f-e9f2adb52a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version 0.3.14\n",
            "Haiku version 0.0.7\n",
            "TF version 2.8.2\n"
          ]
        }
      ],
      "source": [
        "import haiku as hk\n",
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tf.enable_v2_behavior()\n",
        "tf.config.set_visible_devices([], device_type='GPU')\n",
        "\n",
        "print(\"JAX version {}\".format(jax.__version__))\n",
        "print(\"Haiku version {}\".format(hk.__version__))\n",
        "print(\"TF version {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGuFU-DlFMnN"
      },
      "outputs": [],
      "source": [
        "data_dir = '/tmp/tfds'\n",
        "\n",
        "# Fetch full datasets for evaluation\n",
        "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
        "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
        "cifar100_data = tfds.load(name=\"cifar100\", data_dir=data_dir, split=\"train\")\n",
        "\n",
        "def make_dataset(batch_size, seed=1):\n",
        "  def _preprocess(sample):\n",
        "    # Convert to floats in [0, 1].\n",
        "    image = tf.image.convert_image_dtype(sample[\"image\"], tf.float32)\n",
        "    # Scale the data to [-1, 1] to stabilize training.\n",
        "    return 2.0 * image - 1.0\n",
        "  def _label_identity(sample):\n",
        "    label = sample['label']\n",
        "    return label\n",
        "\n",
        "  ds = cifar100_data\n",
        "  #ds = cifar100_data[\"train\"]\n",
        "  ds = ds.map(map_func=_preprocess, \n",
        "              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  ds = ds.cache()\n",
        "  ds = ds.shuffle(10 * batch_size, seed=seed).repeat().batch(batch_size)\n",
        "\n",
        "  labels = cifar100_data\n",
        "  #labels = cifar100_data[\"train\"]\n",
        "  labels = labels.map(map_func=_label_identity, \n",
        "              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  labels = labels.cache()\n",
        "  labels = labels.shuffle(10 * batch_size, seed=seed).repeat().batch(batch_size)\n",
        "  return (iter(tfds.as_numpy(ds)), iter(tfds.as_numpy(labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3Eyr84QZOh3"
      },
      "outputs": [],
      "source": [
        "class ConvAndBatchNormModule(hk.Module):\n",
        "  def __init__(self, is_training, outchannels, kernel_size, stride, name=None):\n",
        "    super(ConvAndBatchNormModule, self).__init__(name=name)\n",
        "    self.conv = hk.Conv2D(outchannels, kernel_size, stride)\n",
        "    self.bn = hk.BatchNorm(True, True, 0.9, cross_replica_axis=\"jax.vmap\")\n",
        "    self.is_training = is_training\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.bn(x, self.is_training)\n",
        "    return x\n",
        "\n",
        "class ResModule(hk.Module):\n",
        "  def __init__(self, is_training, inchannels, adjust_dimension, name=None):\n",
        "    super(ResModule, self).__init__(name=name)\n",
        "    outchannels = inchannels // 4\n",
        "\n",
        "    self.dimensionHelper = None\n",
        "    if adjust_dimension:\n",
        "      self.dimensionHelper = ResDimensionHelper()\n",
        "      outchannels = 2 * outchannels\n",
        "      inchannels = 2 * inchannels\n",
        "\n",
        "    self.conv1 = ConvAndBatchNormModule(is_training, outchannels, 1, 1)\n",
        "\n",
        "    if adjust_dimension:\n",
        "      self.conv2 = ConvAndBatchNormModule(is_training, outchannels, 3, 2)\n",
        "    else:\n",
        "      self.conv2 = ConvAndBatchNormModule(is_training, outchannels, 3, 1)\n",
        "\n",
        "    self.conv3 = ConvAndBatchNormModule(is_training, inchannels, 1, 1)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x_res = x\n",
        "    x = jax.nn.relu(self.conv1(x))\n",
        "    x = jax.nn.relu(self.conv2(x))\n",
        "    x = self.conv3(x)\n",
        "    if self.dimensionHelper != None:\n",
        "      x_res = self.dimensionHelper(x_res)\n",
        "    x = x + x_res\n",
        "    x = jax.nn.relu(x)\n",
        "    return x\n",
        "\n",
        "class ResDimensionHelper(hk.Module):\n",
        "  def __init__(self, name=None):\n",
        "    super(ResDimensionHelper, self).__init__(name=name)\n",
        "    self.maxPool = hk.MaxPool(2, [2,2,1], \"SAME\")\n",
        "\n",
        "  def __call__(self, x):\n",
        "    added_channels = x.shape[2] // 2\n",
        "    x = self.maxPool(x)\n",
        "    x = jnp.pad(x, ((0,0),(0,0),(added_channels, added_channels)))\n",
        "    return x\n",
        "\n",
        "class DownSampleModule(hk.Module):\n",
        "  def __init__(self, name=None):\n",
        "    super(DownSampleModule, self).__init__(name=name)\n",
        "    self.conv = hk.Conv2D(64, 7, 2, padding=\"VALID\")\n",
        "    self.maxPool = hk.MaxPool(3, [2,2,1], \"SAME\")\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = jnp.pad(x, ((3,3),(3,3),(0, 0)))\n",
        "    x = self.conv(x)\n",
        "    x = self.maxPool(x)\n",
        "    return x\n",
        "\n",
        "class GlobalPoolAndFCModule(hk.Module):\n",
        "  def __init__(self, goal_num_classes, name=None):\n",
        "    super(GlobalPoolAndFCModule, self).__init__(name=name)\n",
        "    self.flatten = hk.Flatten(preserve_dims=-3)\n",
        "    self.linear = hk.Linear(goal_num_classes)\n",
        "  def __call__(self, x):\n",
        "    x = hk.avg_pool(x, [x.shape[0], x.shape[1], 1], 1, 'VALID')\n",
        "    x = self.flatten(x)\n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "def softmax_cross_entropy(logits, labels):\n",
        "  one_hot = jax.nn.one_hot(labels, logits.shape[-1])\n",
        "  return jnp.mean(-jnp.sum(jax.nn.log_softmax(logits) * one_hot, axis=-1))\n",
        "\n",
        "def accuracy(logits, labels):\n",
        "  logits = jnp.argmax(logits, axis=1)\n",
        "  return jnp.mean(logits == labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF6QbVfQJD-1"
      },
      "outputs": [],
      "source": [
        "def forward(data, labels, is_training):\n",
        "\n",
        "  my_func = hk.Sequential([DownSampleModule(),\n",
        "                           hk.Conv2D(256, 1, 1),\n",
        "                           ResModule(is_training, 256, False),\n",
        "                           ResModule(is_training, 256, False),\n",
        "                           ResModule(is_training, 256, False),\n",
        "                           ResModule(is_training, 256, True),\n",
        "                           ResModule(is_training, 512, False),\n",
        "                           ResModule(is_training, 512, False),\n",
        "                           ResModule(is_training, 512, False),\n",
        "                           ResModule(is_training, 512, True),\n",
        "                           ResModule(is_training, 1024, False),\n",
        "                           ResModule(is_training, 1024, False),\n",
        "                           ResModule(is_training, 1024, False),\n",
        "                           ResModule(is_training, 1024, False),\n",
        "                           ResModule(is_training, 1024, False),\n",
        "                           ResModule(is_training, 1024, True),\n",
        "                           ResModule(is_training, 2048, False),\n",
        "                           ResModule(is_training, 2048, False),\n",
        "                           GlobalPoolAndFCModule(100)])\n",
        "  logits = jax.vmap(my_func, axis_name=\"jax.vmap\")(data)\n",
        "  loss = softmax_cross_entropy(logits, labels)\n",
        "  acc = accuracy(logits, labels)\n",
        "  return {\"loss\": loss, \"accuracy\": acc}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xypa68fwRAtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37646c3-1641-4c51-d3fe-3f0c7a767296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py:598: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.get_single_element()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py:598: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.get_single_element()`.\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.01\n",
        "input_data, input_labels = make_dataset(1024, seed=1)\n",
        "test_set = tfds.load(name=\"cifar100\", data_dir=data_dir, split=\"test\", batch_size=-1)\n",
        "test_data, test_labels = tfds.as_numpy(test_set[\"image\"]), tfds.as_numpy(test_set[\"label\"])\n",
        "forward = hk.transform_with_state(forward)\n",
        "optimizer = optax.adam(learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sample):\n",
        "    # Convert to floats in [0, 1].\n",
        "    image = tf.image.convert_image_dtype(sample, tf.float32)\n",
        "    # Scale the data to [-1, 1] to stabilize training.\n",
        "    return 2.0 * image - 1.0\n",
        "test_set = tfds.load(name=\"cifar100\", data_dir=data_dir, split=\"test\", batch_size=-1)\n",
        "test_data, test_labels = tfds.as_numpy(preprocess(test_set[\"image\"])), tfds.as_numpy(test_set[\"label\"])"
      ],
      "metadata": {
        "id": "Pbye_jNlpTK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKejg_PDRdeY"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(params, state, opt_state, data, labels):\n",
        "  def adapt_forward(params, state, data, labels):\n",
        "    # Pack model output and state together.\n",
        "    model_output, state = forward.apply(params, state, None, data, labels, True)\n",
        "    loss = model_output[\"loss\"]\n",
        "    return loss, (model_output, state)\n",
        "\n",
        "  grads, (model_output, state) = (jax.grad(adapt_forward, has_aux=True)(params, state, data, labels))\n",
        "\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "\n",
        "  return params, state, opt_state, model_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7UaYPYSRwJo",
        "outputId": "a3f3616e-e0fc-405a-81a2-ed6525b3594e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 100/2000] train loss: 5.644377 train accuracy: 0.031875 test accuracy: 0.066300 \n",
            "[Step 200/2000] train loss: 3.879315 train accuracy: 0.104004 test accuracy: 0.138800 \n",
            "[Step 300/2000] train loss: 3.498480 train accuracy: 0.165078 test accuracy: 0.172900 \n",
            "[Step 400/2000] train loss: 3.401290 train accuracy: 0.182148 test accuracy: 0.196900 \n",
            "[Step 500/2000] train loss: 3.136335 train accuracy: 0.229219 test accuracy: 0.235100 \n",
            "[Step 600/2000] train loss: 2.900311 train accuracy: 0.274678 test accuracy: 0.268500 \n",
            "[Step 700/2000] train loss: 2.668881 train accuracy: 0.320693 test accuracy: 0.286900 \n",
            "[Step 800/2000] train loss: 2.419318 train accuracy: 0.373398 test accuracy: 0.292200 \n",
            "[Step 900/2000] train loss: 2.153525 train accuracy: 0.428926 test accuracy: 0.304300 \n",
            "[Step 1000/2000] train loss: 1.855360 train accuracy: 0.492744 test accuracy: 0.312300 \n",
            "[Step 1100/2000] train loss: 1.507508 train accuracy: 0.575742 test accuracy: 0.313300 \n",
            "[Step 1200/2000] train loss: 1.151454 train accuracy: 0.663047 test accuracy: 0.312900 \n",
            "[Step 1300/2000] train loss: 0.841229 train accuracy: 0.743730 test accuracy: 0.308400 \n",
            "[Step 1400/2000] train loss: 0.593568 train accuracy: 0.815371 test accuracy: 0.312900 \n",
            "[Step 1500/2000] train loss: 0.422070 train accuracy: 0.864160 test accuracy: 0.316300 \n",
            "[Step 1600/2000] train loss: 0.271900 train accuracy: 0.912520 test accuracy: 0.306200 \n",
            "[Step 1700/2000] train loss: 0.256617 train accuracy: 0.916934 test accuracy: 0.317500 \n",
            "[Step 1800/2000] train loss: 0.440927 train accuracy: 0.869248 test accuracy: 0.267600 \n",
            "[Step 1900/2000] train loss: 0.479685 train accuracy: 0.849844 test accuracy: 0.323600 \n",
            "[Step 2000/2000] train loss: 0.093863 train accuracy: 0.971641 test accuracy: 0.338500 \n"
          ]
        }
      ],
      "source": [
        "num_training_updates = 2000\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "rng = jax.random.PRNGKey(42)\n",
        "#with jax.checking_leaks():\n",
        "params, state = forward.init(rng, next(input_data), next(input_labels), True)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "\n",
        "for step in range(1, num_training_updates + 1):\n",
        "  data = next(input_data)\n",
        "  labels = next(input_labels)\n",
        "  params, state, opt_state, train_results = (train_step(params, state, opt_state, data, labels))\n",
        "\n",
        "  train_results = jax.device_get(train_results)\n",
        "  train_losses.append(train_results[\"loss\"])\n",
        "  train_accuracies.append(train_results[\"accuracy\"])\n",
        "\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    model_output, _ = forward.apply(params, state, None, test_data, test_labels, False)\n",
        "    train_step._clear_cache()\n",
        "    #forward.init(rng, next(input_data), next(input_labels), True)\n",
        "\n",
        "    print(f'[Step {step}/{num_training_updates}] ' + \n",
        "          ('train loss: %f ' % np.mean(train_losses[-100:])) + \n",
        "          ('train accuracy: %f ' % np.mean(train_accuracies[-100:])) + \n",
        "          ('test accuracy: %f ' % model_output[\"accuracy\"]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Resnet_jax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEBTyuWfLjhUEh0uLtaQyk",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}